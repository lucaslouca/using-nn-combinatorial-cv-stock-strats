{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d2ce16-0dc4-4bdd-bf6d-1f16d95aacd9",
   "metadata": {},
   "source": [
    "## Install Libraries\n",
    "#### TA-Lib\n",
    "**Outside Docker Jupyter**\n",
    "```bash\n",
    "curl -L http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz -o ta-lib-0.4.0-src.tar.gz && tar -xzf ta-lib-0.4.0-src.tar.gz && cd ta-lib/ && ./configure && make && sudo make install\n",
    "TA_LIBRARY_PATH=/usr/local/lib TA_INCLUDE_PATH=/usr/local/include pip install ta-lib\n",
    "```\n",
    "\n",
    "**Within Docker Jupyter**\n",
    "```bash\n",
    "!curl -L http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz -o ta-lib-0.4.0-src.tar.gz\n",
    "!tar -xzf ta-lib-0.4.0-src.tar.gz\n",
    "%cd ta-lib/\n",
    "!./configure --prefix=$HOME --build=aarch64-unknown-linux-gnu\n",
    "# !./configure --build=x86_64-unknown-linux-gnu\n",
    "!make\n",
    "!make install\n",
    "!TA_LIBRARY_PATH=~/lib TA_INCLUDE_PATH=~/include pip install ta-lib\n",
    "```\n",
    "\n",
    "pip install numpy==1.26.0\n",
    "pip install keras=3.7.0\n",
    "pip install tensorflow=2.18.0\n",
    "pip install pyfolio=0.9.2\n",
    "pip install hurst=0.0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2a647-097d-4f71-bac6-9e96b4e7ed5b",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b81c44-5685-4383-af48-89b41d099604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib\n",
    "\n",
    "# Building the Artificial Neural Network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from itertools import combinations\n",
    "import itertools as itt\n",
    "import pyfolio as pf\n",
    "from tqdm.notebook import tqdm\n",
    "from hurst import compute_Hc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2198eb-3016-4255-8af1-ed1e8cf54a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed to a fixed number\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ce05f-842a-43ab-8821-d355afa6a3f4",
   "metadata": {},
   "source": [
    "## Helper Functions for CCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c648c32-15e1-46a4-b145-71e88d4cfc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpcv_generator(t_span, num_groups, k, verbose=True):\n",
    "    # split data into N groups, with N << T\n",
    "    # this will assign each index position to a group position\n",
    "    group_num = np.arange(t_span) // (t_span // num_groups)\n",
    "    group_num[group_num == num_groups] = num_groups-1\n",
    "    \n",
    "    # generate the combinations \n",
    "    test_groups = np.array(list(itt.combinations(np.arange(num_groups), k))).reshape(-1, k) # 15×2 matrix: [0,1],[0,2]...[0,5]...[3 5],[4 5]]\n",
    "    C_nk = len(test_groups)\n",
    "    n_paths = C_nk * k // num_groups \n",
    "    \n",
    "    if verbose:\n",
    "        print('n_sim:', C_nk)\n",
    "        print('n_paths:', n_paths)\n",
    "    \n",
    "    # is_test is a T x C(n, k) array where each column is a logical array \n",
    "    # indicating which observation is in the test set\n",
    "    is_test_group = np.full((num_groups, C_nk), fill_value=False) # 6×15 matrix\n",
    "    is_test = np.full((t_span, C_nk), fill_value=False) # 100×15 matrix or 7566×15 matrix\n",
    "\n",
    "    # assign test folds for each of the C(n, k) simulations\n",
    "    for sim_idx, pair in enumerate(test_groups):\n",
    "        i, j = pair # [0,1], [0,2],... [0,5], [1,2],..., [4,5]\n",
    "        is_test_group[[i, j], sim_idx] = True # is_test_group is 6×15 matrix. \n",
    "                                              # Since test_groups is [0,1], [0,2], [0,3], [0,4],... [0,5], [1,2],..., [4,5], simulation with idx 3 will consists of ticks in group 0 and 4\n",
    "        # assigning the test folds\n",
    "        mask = (group_num == i) | (group_num == j) # group_num = [0, 0, 0, 0, ..., 1, 1, ..., 2, 2, 2, 2, ..., 3, 3, ..., 4, 4, 4, 4, ...] of length 100 or 7566 (number of ticks)\n",
    "                                                   # for [i,j]=[0,4] mask becomes [True, True, True, True, ..., False, False, ..., False, False, False, False, ..., False, False, ..., True, True, True, True, ...]\n",
    "        is_test[mask, sim_idx] = True # Mark the rows that belong to group i,j so that they belong to sim_idx and are for testing and backtesting.\n",
    "                                      # For example, since groups [0,4] belong to simulation 3, we set simulation 3 to True for all the rows that belong to group 0 or 4 to indicate that they should be used\n",
    "                                      # for testing in simulation 3\n",
    "\n",
    "    # for each path, connect the folds from different simulations to form a test path\n",
    "    # the fold coordinates are: the fold number, and the simulation index e.g. simulation 0, fold 0 etc\n",
    "    path_folds = np.full((num_groups, n_paths), fill_value=np.nan)\n",
    "    for p in range(n_paths):\n",
    "        for group in range(num_groups):\n",
    "            # If is_test_group[group, :]=[F, F, F, T, F, F, F, T, F, F, T, F, T, F, T] then sim_idx=3\n",
    "            sim_idx = is_test_group[group, :].argmax().astype(int)\n",
    "            path_folds[group, p] = sim_idx # Considering the above example where sim_idx=3 is associated with groups [0,4]: path_folds[0,0], path_folds[0,1],..., path_folds[0,4] will be set to sim_idx=3\n",
    "                                           # Same for group 4: path_folds[4,0], path_folds[4,1],..., path_folds[4,4] will be set to sim_idx=3\n",
    "\n",
    "            is_test_group[group, sim_idx] = False # Mark it as False so on the next iteration we get the next sim_idx when doing a \"...argmax().astype(int)\" (e.g. sim_idx=7)\n",
    "            \n",
    "    \n",
    "    # finally, for each path we indicate which simulation we're building the path from and the time indices\n",
    "    paths = np.full((t_span, n_paths), fill_value= np.nan) # 100×15 matrix or 7566×5 matrix\n",
    "    for p in range(n_paths):\n",
    "        for g in range(num_groups):\n",
    "            mask = (group_num == g) # Get all the ticks that belong to group g\n",
    "            paths[mask, p] = int(path_folds[g, p])\n",
    "    # paths = paths_# .astype(int)\n",
    "\n",
    "    # Once done the matrices will look like so:\n",
    "    # is_test[99] = [F, F, F, F, True, F, F, F, True, F, F, True, F, True, True]\n",
    "    # paths[99] = [4, 8, 11, 13, 14]\n",
    "    # path_folds[5] = [4, 8, 11, 13, 14]\n",
    "    return (is_test, paths, path_folds)\n",
    "\n",
    "# AFML, snippet 7.1\n",
    "# purging removes from the training set those samples that are build with information that \n",
    "# overlaps samples in the testing set.\n",
    "def purge(t1, test_times): # whatever is not in the train set should be in the test set\n",
    "    train = t1.copy(deep=True) # copy of the index\n",
    "    for test_start, test_end in test_times.items():\n",
    "        df_0 = train[(test_start <= train.index) & (train.index <= test_end)].index # train starts within test\n",
    "        df_1 = train[(test_start <= train) & (train <= test_end)].index # train ends within test\n",
    "        df_2 = train[(train.index <= test_start) & (test_end <= train)].index # train envelopes test\n",
    "        train = train.drop(df_0.union(df_1).union(df_2))\n",
    "    return train\n",
    "\n",
    "# AFML, snippet 7.2\n",
    "def embargo_(times, pct_embargo):\n",
    "    step = int(times.shape[0] * pct_embargo) # more complicated logic if needed to use a time delta\n",
    "    if step == 0:\n",
    "        ans = pd.Series(times, index=test_times)\n",
    "    else:\n",
    "        ans = pd.Series(times[step:].values, index=times[:-step].index)\n",
    "        ans = pd.concat([ans, pd.Series(times.iloc[-1], index=times[-step:].index)])\n",
    "    return ans\n",
    "\n",
    "# embargo removes a number of observations from the end of the test set.\n",
    "def embargo(test_times, t1, pct_embargo=0.01): # done before purging\n",
    "    # embargoed t1\n",
    "    t1_embargo = embargo_(t1, pct_embargo)\n",
    "    test_start, test_end = test_times.index[0], test_times.index[-1]\n",
    "    #print(f\"Test start/end times:{test_start}/{test_end}\")\n",
    "    test_times_embargoed = t1_embargo.loc[test_times.index]\n",
    "    return test_times_embargoed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb21d4-3c99-480e-9afb-02019c52a5ff",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11ecbf-b3fc-49da-9bee-8d7c7a54cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from CSV file\n",
    "data = pd.read_csv('aapl_prices.csv', parse_dates=True, sep=',', index_col=[0])\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076fb28-dee7-4d3d-955e-9822bbd5a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_data = pd.read_csv('spy_prices.csv', parse_dates=True, sep=',', index_col=[0])\n",
    "spy_data = spy_data.loc[data.index]\n",
    "spy_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef32cb-cf15-409b-987e-6ac3285eadd3",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f87ca1-4d37-43d8-bcd6-2d965c4c5bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "data['H-L'] = data['High'] - data['Low']\n",
    "data['O-C'] = data['Close'] - data['Open']\n",
    "data['3day MA'] = data['Close'].shift(1).rolling(window = 3).mean()\n",
    "data['10day MA'] = data['Close'].shift(1).rolling(window = 10).mean()\n",
    "data['30day MA'] = data['Close'].shift(1).rolling(window = 30).mean()\n",
    "data['21day MA'] = data['Close'].shift(1).rolling(21).mean()\n",
    "data['263day MA'] = data['Close'].shift(1).rolling(262).mean()\n",
    "data['Std_dev']= data['Close'].rolling(5).std()\n",
    "data['RSI'] = talib.RSI(data['Close'].values, timeperiod = 9)\n",
    "data['Williams %R'] = talib.WILLR(data['High'].values, data['Low'].values, data['Close'].values, 7)\n",
    "data['zscore'] = (data.Close - data.Close.rolling(126).mean())/data.Close.rolling(126).std()\n",
    "data['hurst'] = data['Close'].rolling(window=126).apply(lambda x: compute_Hc(x)[0])\n",
    "\n",
    "data['High_std'] = (data['High']-data['High'].mean())/data['High'].std() \n",
    "spy_data['High_std'] = (spy_data['High']-spy_data['High'].mean())/spy_data['High'].std() \n",
    "data['aapl-spy'] = data['High_std'] - spy_data['High_std']\n",
    "\n",
    "# 21 days holding period\n",
    "data['returns'] = data.Close.shift(-21).ffill().pct_change(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c0c18-0a67-42b8-ade1-b3895e2bc6d6",
   "metadata": {},
   "source": [
    "## Defining input features from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69380daf-018c-4023-b240-c9550dde93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Price_Rise'] = np.where(data['Close'].shift(-21) > data['Close'], 1, 0)\n",
    "data = data.dropna()\n",
    "\n",
    "feats = ['zscore', 'aapl-spy']\n",
    "#feats = ['zscore', 'Std_dev', '21day MA', '263day MA']\n",
    "X = data[feats]\n",
    "y = data['Price_Rise']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee5fc29-b634-49fe-bdee-a01b223b3b3e",
   "metadata": {},
   "source": [
    "## Building the artificial neural network model\n",
    "\n",
    "### is_test\n",
    "Has _tick-number_ of rows, with each row pointing to a _number-of-sims_ long array, marking the simulation to be used as True. \n",
    "For example assuming: \n",
    "```\n",
    "...\n",
    "is_test[50] = [F, F, T, F, F, F, T, F, F, F, F, F, T, T, F]\n",
    "...\n",
    "is_test[99] = [F, F, F, F, T, F, F, F, T, F, F, T, F, T, T]\n",
    "...\n",
    "```\n",
    "above, we read columnwise for each of the 15 simulations. In simulation index 2, tick number 50 will be considered for testing/backtesting, while tick 99 will not. On the other hand in simulation 4 tick 99 will be used for test while tick 50 will not.\n",
    "\n",
    "### paths\n",
    "Has _tick-number_ of rows and 5 columns. Each column represents one path (again, we read column wise) . A path consist of simulation indices. Example: `paths[99] = [4, 8, 11, 13, 14]`. In this example, tick 99 is part of 5 paths. In the first path we will consider the prediction of simulation 4. In the second path we will consider the prediction of simulation 8, and so on. It will be used for the backtesting after we have trained (using training set) and predicted using the test set. In other words, if our prediction (`pred`) matches the real signal (`old_signal`) then `backtest_paths[tick, p]` will hold the return that we would gain. This logic is depicted below:\n",
    "```\n",
    "for p in range(paths.shape[1]):\n",
    "    for t, sim in enumerate(paths[:, p]): # index, value\n",
    "        backtest_paths[t, p] = ( bool(pred[t, int(sim)]) &  bool(old_signal.iloc[t]) ) * ret.iloc[t]\n",
    "```\n",
    "`pred` has _tick-number_ of rows and _number-of-sims_ columns filled as follows: \n",
    "```\n",
    "for sim in range(num_sim):\n",
    "    test_idx = is_test[:, sim] \n",
    "    ...\n",
    "    pred[test_idx, sim] = classifier.predict(X_test)\n",
    "```\n",
    "\n",
    "<img src=\"CCV.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f693d0-a395-4da3-b6f7-483366a4ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction and evaluation times\n",
    "# using business days, but the index is not holidays aware -- it can be fixed\n",
    "t1_ = data.index\n",
    "\n",
    "# we are holding our position for 21 days\n",
    "t1 = pd.Series(t1_[21:], index=t1_[:-21]) # t1 is both the trade time and the event time\n",
    "\n",
    "# realign data\n",
    "data = data.loc[t1.index]\n",
    "\n",
    "data = data.dropna()\n",
    "# realign t1\n",
    "t1 = t1.loc[data.index]\n",
    "\n",
    "num_paths = 5\n",
    "num_groups_test = 2\n",
    "num_groups = num_paths + 1 \n",
    "num_ticks = len(data)\n",
    "is_test, paths, _ = cpcv_generator(num_ticks, num_groups, num_groups_test)\n",
    "pred = np.full(is_test.shape, np.nan)\n",
    "\n",
    "num_sim = is_test.shape[1] # num of simulations needed to generate all backtest paths\n",
    "for sim in tqdm(range(num_sim)):\n",
    "    # get train set|\n",
    "    test_idx = is_test[:, sim] \n",
    "    \n",
    "    # convert numerical indices into time stamps\n",
    "    test_times = t1.loc[test_idx]\n",
    "    \n",
    "    # embargo\n",
    "    test_times_embargoed = embargo(test_times, t1, pct_embargo=0.01)\n",
    "    \n",
    "    # purge\n",
    "    train_times = purge(t1, test_times_embargoed)\n",
    "    \n",
    "    # split training / test sets\n",
    "    X_test = X.loc[test_times.index, :]\n",
    "    y_test = y.loc[X_test.index]\n",
    "    \n",
    "    X_train = X.loc[train_times.index, :]\n",
    "    y_train = y.loc[X_train.index]\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    # reconstructing the backtest paths\n",
    "    print(f'training classifier for simulation %s' % sim)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # https://stackoverflow.com/questions/45393429/keras-how-to-save-model-and-continue-training\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 128, kernel_initializer = 'uniform', activation = 'relu', input_dim = X.shape[1]))\n",
    "    classifier.add(Dense(units = 128, kernel_initializer = 'uniform', activation = 'relu')) # second layer\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) # output layer\n",
    "    classifier.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy']) #  ‘adam’ is stochastic gradient descent.\n",
    "    classifier.fit(X_train, y_train, batch_size = 10, epochs = 100, verbose=0)\n",
    "\n",
    "    pred_ = classifier.predict(X_test)\n",
    "    pred_ = np.concatenate(pred_, axis=0)\n",
    "\n",
    "    pred_ = (pred_ > 0.5)\n",
    "    \n",
    "    # fill the backtesting prediction matrix\n",
    "    pred[test_idx, sim] = pred_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0138b92-5b6f-42cd-a442-ebff0af468f8",
   "metadata": {},
   "source": [
    "## Computation of strategy returns and determine trade positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05210c24-ab7a-4e5b-8e1d-d6675420bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_paths = np.full((paths.shape[0], paths.shape[1]), np.nan)\n",
    "\n",
    "for p in range(paths.shape[1]):\n",
    "    for t, sim in enumerate(paths[:, p]): # index, value\n",
    "        backtest_paths[t, p] =  np.where(pred[t, int(sim)] == True, data.iloc[t]['returns'], -data.iloc[t]['returns'])\n",
    "        \n",
    "import pyfolio as pf\n",
    "perf_func = pf.timeseries.perf_stats\n",
    "\n",
    "perf_list = []\n",
    "for s in range(paths.shape[1]):\n",
    "    perf_table = perf_func(backtest_paths[:, s])\n",
    "    perf_list.append(perf_table)\n",
    "\n",
    "perf_paths = pd.concat(perf_list, axis=1)\n",
    "perf_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082dfd91-14d4-46ff-a243-49b7060df97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_paths.loc['Sharpe ratio', :].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c0c5b-7123-4b33-afeb-7d0cf06c0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_paths.loc['Sharpe ratio', :].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46a4e1-701a-4ab1-a4b5-a71be8e84604",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_paths.loc['Sharpe ratio', :].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a15a1-10b9-4592-9aa7-a0db4cbfd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_paths.loc['Max drawdown', :].mean() # slightly better than the base strategy -- but still very risky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532182cd-6af8-456f-adf3-fd09338ab5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_paths.loc['Max drawdown', :].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d01ab-4aa6-405a-b892-3ef13c5e70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_paths.loc['Max drawdown', :].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a488ff-a057-4617-985d-2beaa2236ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cumulative_market_returns'] = np.cumsum(data['returns'])\n",
    "\n",
    "strategy_returns = np.full((paths.shape[0], paths.shape[1]), np.nan)\n",
    "for p in range(paths.shape[1]):\n",
    "    for t, sim in enumerate(paths[:, p]): # index, value\n",
    "        strategy_returns[t, p] =  np.where(pred[t, int(sim)] == True, data.iloc[t]['returns'], -data.iloc[t]['returns'])\n",
    "\n",
    "for p in range(paths.shape[1]):\n",
    "    data[f'cumulative_strategy_returns_{p}'] = np.cumsum(strategy_returns[:, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7d9b3-f3d3-478c-b3ac-d7916611f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(data['cumulative_market_returns'], color='g', label='Market Returns')\n",
    "\n",
    "color = iter(plt.colormaps.get_cmap('viridis').resampled(10).colors)\n",
    "for p in range(paths.shape[1]):\n",
    "    c = next(color)\n",
    "    plt.plot(data[f'cumulative_strategy_returns_{p}'], color=c, label=f'Strategy Returns [path {p}]',linewidth=0.5)\n",
    "\n",
    "plt.title('Market Returns and Strategy Returns', color='purple', size=15)\n",
    "\n",
    "# Setting axes labels for close prices plot\n",
    "plt.xlabel('Dates', {'color': 'orange', 'fontsize':15})\n",
    "plt.ylabel('Returns(%)', {'color': 'orange', 'fontsize':15})\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76dffd9-5e0c-4b9f-9969-55033edb34b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
